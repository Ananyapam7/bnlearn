<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Working with Continous data &mdash; bnlearn bnlearn documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=e0179649" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=441dd29d"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Predict" href="Predict.html" />
    <link rel="prev" title="Inference" href="Inference.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            bnlearn
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
    
              <p class="caption" role="heading"><span class="caption-text">Quickstart</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Quickstart.html">Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Installation.html">Install from Pypi (pip)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Installation.html#install-from-github">Install from github</a></li>
<li class="toctree-l1"><a class="reference internal" href="Installation.html#create-environment">Create environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="Installation.html#uninstall">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="Installation.html#validate">Validate</a></li>
<li class="toctree-l1"><a class="reference internal" href="Installation.html#import-error">Import Error</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Structure learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html">Causation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#exhaustivesearch">Exhaustivesearch</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#hillclimbsearch">Hillclimbsearch</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#chow-liu">Chow-liu</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#tree-augmented-naive-bayes-tan">Tree-augmented Naive Bayes (TAN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#naivebayes">NaiveBayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure%20learning.html#constraint-based">Constraint-based</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parameter learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Parameter%20learning.html">Parameter learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameter%20learning.html#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameter%20learning.html#bayesian-parameter-estimation">Bayesian Parameter Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameter%20learning.html#examples-parameter-learning">Examples Parameter learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Parameter%20learning.html#conditional-probability-distributions-cpd">Conditional Probability Distributions (CPD)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="Inference.html#inference-algorithms">Inference Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Inference.html#examples-inference">Examples Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Continuous Data</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Working with Continous data</a></li>
<li class="toctree-l1"><a class="reference internal" href="#discretize-continuous-datasets-manually">Discretize continuous datasets manually</a></li>
<li class="toctree-l1"><a class="reference internal" href="#discretize-continuous-datasets-automatically">Discretize continuous datasets automatically</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#structure-learning">Structure learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#parameter-learning">Parameter learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inferences">Inferences</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#modelling-continuous-datasets">Modelling Continuous Datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#lingam-based-methods">LiNGAM-based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#direct-lingam-method">Direct-LiNGAM method</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ica-lingam-method">ICA-LiNGAM method</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pc-method">PC method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Predict</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Predict.html">Predict</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sampling</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Sampling.html">Forward Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sampling.html#gibbs-sampling">Gibbs Sampling</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Plot</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Plot.html">Interactive plot</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot.html#static-plot-networkx">Static plot (networkx)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot.html#static-plot-graphviz">Static plot (graphviz)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot.html#comparison-of-two-networks">Comparison of two networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot.html#node-properties">Node properties</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot.html#edge-properties">Edge properties</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other functionalities</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="independence_test.html">Independence test</a></li>
<li class="toctree-l1"><a class="reference internal" href="Create%20DAG.html">Directed Acyclic Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="Example%20Datasets.html">DataFrames</a></li>
<li class="toctree-l1"><a class="reference internal" href="Example%20Datasets.html#import-dag-bif">Import DAG/BIF</a></li>
<li class="toctree-l1"><a class="reference internal" href="Example%20Datasets.html#export-dag-bif">Export DAG/BIF</a></li>
<li class="toctree-l1"><a class="reference internal" href="whitelist_blacklist.html">Black and white lists</a></li>
<li class="toctree-l1"><a class="reference internal" href="topological_sort.html">Topological sort</a></li>
<li class="toctree-l1"><a class="reference internal" href="dataframe%20conversions.html">Data Conversions</a></li>
<li class="toctree-l1"><a class="reference internal" href="Structure_scores.html">Structure Scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="saving%20and%20loading.html">Saving and Loading</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Examples.html">Start with RAW data</a></li>
<li class="toctree-l1"><a class="reference internal" href="Examples.html#structure-learning">Structure learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Examples.html#parameter-learning">Parameter learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Examples.html#create-a-bayesian-network-learn-its-parameters-from-data-and-perform-the-inference">Create a Bayesian Network, learn its parameters from data and perform the inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="UseCases.html">Use Case Titanic</a></li>
<li class="toctree-l1"><a class="reference internal" href="UseCases.html#use-case-medical-domain">Use Case Medical domain</a></li>
<li class="toctree-l1"><a class="reference internal" href="UseCases.html#use-case-continuous-datasets">Use Case Continuous Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parameters and attributes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bnlearn.structure_learning.html">bnlearn.structure_learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="bnlearn.parameter_learning.html">bnlearn.parameter_learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="bnlearn.inference.html">bnlearn.inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="bnlearn.bnlearn.html">bnlearn.bnlearn</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html">Sponsor</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#medium-blog">Medium Blog</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#github">Github</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#colab-notebook">Colab Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#citing">Citing</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#references">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="Documentation.html#related-packages">Related Packages</a></li>
</ul>

    <a href= "genindex.html">Index</a>
  
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">bnlearn</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Working with Continous data</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/Continuous Data.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="working-with-continous-data">
<h1>Working with Continous data<a class="headerlink" href="#working-with-continous-data" title="Link to this heading"></a></h1>
<p>Learning Bayesian Networks from continuous data is a challanging task.
There are different manner on how to work with continuous and/or hybrid datasets. Each manner has their own advantages and disadvantages.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">bnlearn</span></code> the following options are available to work with continuous datasets:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>Discretize continuous datasets manually using domain knowledge.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>Discretize continuous datasets using a principled Bayesian discretization method.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>Model continuous and hybrid datasets in a semi-parametric approach that assumes a linear relationships.</p></li>
</ol>
</li>
</ul>
</section>
<section id="discretize-continuous-datasets-manually">
<h1>Discretize continuous datasets manually<a class="headerlink" href="#discretize-continuous-datasets-manually" title="Link to this heading"></a></h1>
<p>Discretizing continuous datasets manually using domain knowledge involves dividing a continuous variable into a set of discrete intervals based on an understanding of the data’s context and the relationships between variables. This method allows for meaningful groupings of data points, which can simplify analysis and improve interpretability in models.</p>
<p>By leveraging expertise in the subject matter, the intervals or thresholds can be chosen to reflect real-world significance, such as categorizing weather conditions into meaningful ranges (e.g., “freezing,” “warm,” “hot”). This approach contrasts with automatic binning methods (as depicted in approach 2), such as equal-width or equal-frequency binning, where intervals may not correspond to meaningful domain-specific boundaries.</p>
<p>For instance, lets load the auto mpg data set and based on automotive standards, we can define horsepower categories:</p>
<ul class="simple">
<li><p>Low: Cars with horsepower less than 100 (typically small, fuel-efficient cars)</p></li>
<li><p>Medium: Cars with horsepower between 100 and 150 (moderate performance cars)</p></li>
<li><p>High: Cars with horsepower above 150 (high-performance vehicles)</p></li>
</ul>
<p>After all continuous variables are catagorized, the normal structure learning procedure can be applied.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="nn">bn</span>

<span class="c1"># Load data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">import_example</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;auto_mpg&#39;</span><span class="p">)</span>

<span class="c1"># Print</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="c1">#       mpg  cylinders  displacement  ...  acceleration  model_year  origin</span>
<span class="c1"># 0    18.0          8         307.0  ...          12.0          70       1</span>
<span class="c1"># 1    15.0          8         350.0  ...          11.5          70       1</span>
<span class="c1"># 2    18.0          8         318.0  ...          11.0          70       1</span>
<span class="c1"># 3    16.0          8         304.0  ...          12.0          70       1</span>
<span class="c1"># 4    17.0          8         302.0  ...          10.5          70       1</span>
<span class="c1"># ..    ...        ...           ...  ...           ...         ...     ...</span>
<span class="c1"># 387  27.0          4         140.0  ...          15.6          82       1</span>
<span class="c1"># 388  44.0          4          97.0  ...          24.6          82       2</span>
<span class="c1"># 389  32.0          4         135.0  ...          11.6          82       1</span>
<span class="c1"># 390  28.0          4         120.0  ...          18.6          82       1</span>
<span class="c1"># 391  31.0          4         119.0  ...          19.4          82       1</span>
<span class="c1">#</span>
<span class="c1"># [392 rows x 8 columns]</span>

<span class="c1"># Define horsepower bins based on domain knowledge</span>
<span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;low&#39;</span><span class="p">,</span> <span class="s1">&#39;medium&#39;</span><span class="p">,</span> <span class="s1">&#39;high&#39;</span><span class="p">]</span>

<span class="c1"># Discretize horsepower using the defined bins</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;horsepower_category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;horsepower&#39;</span><span class="p">,</span> <span class="s1">&#39;horsepower_category&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="c1">#    horsepower horsepower_category</span>
<span class="c1"># 0       130.0              medium</span>
<span class="c1"># 1       165.0                high</span>
<span class="c1"># 2       150.0              medium</span>
<span class="c1"># 3       150.0              medium</span>
<span class="c1"># 4       140.0              medium</span>
</pre></div>
</div>
</section>
<section id="discretize-continuous-datasets-automatically">
<h1>Discretize continuous datasets automatically<a class="headerlink" href="#discretize-continuous-datasets-automatically" title="Link to this heading"></a></h1>
<p>Automatic discritizing datasets is accomplished by using a principled Bayesian discretization method.
The method is created by Yi-Chun Chen et al in Julia. The code is ported to Python and is now part of <code class="docutils literal notranslate"><span class="pre">bnlearn</span></code>.
Yi-Chun Chen demonstrates that his proposed method is superior to the established minimum description length algorithm.
A disadvantage of this approach is that you need to pre-define the edges before you can apply the discritization method.
The underlying idea is that after applying this discritization method, structure learning approaches can then be applied.
To demonstrate the usage of automatically discritizing continuous data, lets use the <strong>auto mpg</strong> dataset again.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="nn">bn</span>

<span class="c1"># Load data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">import_example</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;auto_mpg&#39;</span><span class="p">)</span>
<span class="c1"># Print</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1">#       mpg  cylinders  displacement  ...  acceleration  model_year  origin</span>
<span class="c1"># 0    18.0          8         307.0  ...          12.0          70       1</span>
<span class="c1"># 1    15.0          8         350.0  ...          11.5          70       1</span>
<span class="c1"># 2    18.0          8         318.0  ...          11.0          70       1</span>
<span class="c1"># 3    16.0          8         304.0  ...          12.0          70       1</span>
<span class="c1"># 4    17.0          8         302.0  ...          10.5          70       1</span>
<span class="c1"># ..    ...        ...           ...  ...           ...         ...     ...</span>
<span class="c1"># 387  27.0          4         140.0  ...          15.6          82       1</span>
<span class="c1"># 388  44.0          4          97.0  ...          24.6          82       2</span>
<span class="c1"># 389  32.0          4         135.0  ...          11.6          82       1</span>
<span class="c1"># 390  28.0          4         120.0  ...          18.6          82       1</span>
<span class="c1"># 391  31.0          4         119.0  ...          19.4          82       1</span>
<span class="c1">#</span>
<span class="c1"># [392 rows x 8 columns]</span>

<span class="c1"># Define the edges</span>
<span class="n">edges</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;cylinders&quot;</span><span class="p">,</span> <span class="s2">&quot;displacement&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;displacement&quot;</span><span class="p">,</span> <span class="s2">&quot;model_year&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;displacement&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;displacement&quot;</span><span class="p">,</span> <span class="s2">&quot;horsepower&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;model_year&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;mpg&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;horsepower&quot;</span><span class="p">,</span> <span class="s2">&quot;acceleration&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;mpg&quot;</span><span class="p">,</span> <span class="s2">&quot;model_year&quot;</span><span class="p">),</span>
<span class="p">]</span>

<span class="c1"># Create DAG based on edges</span>
<span class="n">DAG</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">make_DAG</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span>

<span class="c1"># Plot the DAG</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">DAG</span><span class="p">)</span>

<span class="c1"># Plot the DAG using graphviz</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">DAG</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-auto-mpg-dag-edges">
<img alt="_images/auto_mpg_DAG_edges.png" src="_images/auto_mpg_DAG_edges.png" />
</figure>
<p>We can now discretize the continuous columns as following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A good habbit is to set the columns with continuous data as float</span>
<span class="n">continuous_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mpg&quot;</span><span class="p">,</span> <span class="s2">&quot;displacement&quot;</span><span class="p">,</span> <span class="s2">&quot;horsepower&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;acceleration&quot;</span><span class="p">]</span>

<span class="c1"># Discretize the continous columns by specifying</span>
<span class="n">df_discrete</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">discretize</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">continuous_columns</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#                 mpg  cylinders  ... model_year origin</span>
<span class="c1"># 0     (17.65, 21.3]          8  ...         70      1</span>
<span class="c1"># 1    (8.624, 15.25]          8  ...         70      1</span>
<span class="c1"># 2     (17.65, 21.3]          8  ...         70      1</span>
<span class="c1"># 3    (15.25, 17.65]          8  ...         70      1</span>
<span class="c1"># 4    (15.25, 17.65]          8  ...         70      1</span>
<span class="c1"># ..              ...        ...  ...        ...    ...</span>
<span class="c1"># 387   (25.65, 28.9]          4  ...         82      1</span>
<span class="c1"># 388    (28.9, 46.6]          4  ...         82      2</span>
<span class="c1"># 389    (28.9, 46.6]          4  ...         82      1</span>
<span class="c1"># 390   (25.65, 28.9]          4  ...         82      1</span>
<span class="c1"># 391    (28.9, 46.6]          4  ...         82      1</span>
<span class="c1">#</span>
<span class="c1"># [392 rows x 8 columns]</span>
</pre></div>
</div>
<p>At this point it is not different than any other discrete data set. We can specify the DAG together with the
discrete data frame and fit a model using <code class="docutils literal notranslate"><span class="pre">bnlearn</span></code>.</p>
<section id="structure-learning">
<h2>Structure learning<a class="headerlink" href="#structure-learning" title="Link to this heading"></a></h2>
<p>We will learn the structure on the continuous data. Note that the data is also discretezed on a set of edges which will
likely introduce a bias in the learned structure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learn the structure</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">structure_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_discrete</span><span class="p">,</span> <span class="n">methodtype</span><span class="o">=</span><span class="s1">&#39;hc&#39;</span><span class="p">,</span> <span class="n">scoretype</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">)</span>

<span class="c1"># Independence test</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># [bnlearn] &gt;Compute edge strength with [chi_square]</span>
<span class="c1"># [bnlearn] &gt;Edge [weight &lt;-&gt; mpg] [P=0.999112] is excluded because it was not significant (P&lt;0.05) with [chi_square]</span>

<span class="c1"># Make plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>

<span class="c1"># Make plot with graphviz</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>

<span class="c1"># Create interactive plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig2a" src="_images/fig2a.png" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="fig2b" src="_images/fig2b.png" /></p></td>
</tr>
</tbody>
</table>
<iframe src="https://erdogant.github.io/docs/d3blocks/bnlearn_continous_example_1.html" height="700px" width="750px", frameBorder="0"></iframe></section>
<section id="parameter-learning">
<h2>Parameter learning<a class="headerlink" href="#parameter-learning" title="Link to this heading"></a></h2>
<p>Let’s continue with parameter learning on the continuous data set and see whether we can estimate the CPDs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit model based on DAG and discretized continous columns</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">parameter_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">DAG</span><span class="p">,</span> <span class="n">df_discrete</span><span class="p">)</span>

<span class="c1"># Use MLE method</span>
<span class="c1"># model_mle = bn.parameter_learning.fit(DAG, df_discrete, methodtype=&quot;maximumlikelihood&quot;)</span>
</pre></div>
</div>
<p>After fitting the model on the DAG and data frame, we can perform the independence test to remove any spurious edges and
create a plot. In this case, the tooltips will contain the CPDs as these are computed with parameter learning.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Independence test</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Make plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>

<span class="c1"># Make plot graphviz</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>

<span class="c1"># Create interactive plot.</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig3a" src="_images/fig_cont_1.png" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="fig3b" src="_images/fig_cont_1b.png" /></p></td>
</tr>
</tbody>
</table>
<iframe src="https://erdogant.github.io/docs/d3blocks/bnlearn_continous_example_2.html" height="700px" width="750px", frameBorder="0"></iframe><p>There are various manners to deeper investigate the results such as looking at the CPDs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print CPDs</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_cpds</span><span class="p">(</span><span class="s2">&quot;mpg&quot;</span><span class="p">))</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>weight</p></td>
<td><p>…</p></td>
<td><p>weight((3657.5, 5140.0])</p></td>
</tr>
<tr class="row-even"><td><p>mpg((8.624, 15.25])</p></td>
<td><p>…</p></td>
<td><p>0.29931972789115646</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((15.25, 17.65])</p></td>
<td><p>…</p></td>
<td><p>0.19727891156462582</p></td>
</tr>
<tr class="row-even"><td><p>mpg((17.65, 21.3])</p></td>
<td><p>…</p></td>
<td><p>0.13313896987366375</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((21.3, 25.65])</p></td>
<td><p>…</p></td>
<td><p>0.12439261418853255</p></td>
</tr>
<tr class="row-even"><td><p>mpg((25.65, 28.9])</p></td>
<td><p>…</p></td>
<td><p>0.12439261418853255</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((28.9, 46.6])</p></td>
<td><p>…</p></td>
<td><p>0.12147716229348882</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weight categories: &quot;</span><span class="p">,</span> <span class="n">df_disc</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">categories</span><span class="p">)</span>
<span class="c1"># Weight categories:  IntervalIndex([(1577.73, 2217.0], (2217.0, 2959.5], (2959.5, 3657.5], (3657.5, 5140.0]], dtype=&#39;interval[float64, right]&#39;)</span>
</pre></div>
</div>
</section>
<section id="inferences">
<h2>Inferences<a class="headerlink" href="#inferences" title="Link to this heading"></a></h2>
<p>Making inferences can be perfomred using the fitted model. Note that the evidence should be discretized for which we can
use the <code class="docutils literal notranslate"><span class="pre">discretize_value</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">evidence</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">bn</span><span class="o">.</span><span class="n">discretize_value</span><span class="p">(</span><span class="n">df_discrete</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">],</span> <span class="mf">3000.0</span><span class="p">)}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">evidence</span><span class="p">)</span>
<span class="c1"># {&#39;weight&#39;: Interval(2959.5, 3657.5, closed=&#39;right&#39;)}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">bn</span><span class="o">.</span><span class="n">inference</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mpg&quot;</span><span class="p">],</span> <span class="n">evidence</span><span class="o">=</span><span class="n">evidence</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>mpg</p></th>
<th class="head"><p>phi(mpg)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mpg((8.624, 15.25])</p></td>
<td><p>0.1510</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((15.25, 17.65])</p></td>
<td><p>0.1601</p></td>
</tr>
<tr class="row-even"><td><p>mpg((17.65, 21.3])</p></td>
<td><p>0.2665</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((21.3, 25.65])</p></td>
<td><p>0.1540</p></td>
</tr>
<tr class="row-even"><td><p>mpg((25.65, 28.9])</p></td>
<td><p>0.1327</p></td>
</tr>
<tr class="row-odd"><td><p>mpg((28.9, 46.6])</p></td>
<td><p>0.1358</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="modelling-continuous-datasets">
<h1>Modelling Continuous Datasets<a class="headerlink" href="#modelling-continuous-datasets" title="Link to this heading"></a></h1>
<section id="lingam-based-methods">
<h2>LiNGAM-based Methods<a class="headerlink" href="#lingam-based-methods" title="Link to this heading"></a></h2>
<p>Bnlearn includes LiNGAM-based methods which do the estimation of Linear, Non-Gaussian Acyclic Model from observed data. It assumes non-Gaussianity of the noise terms in the causal model.
Various methods are developed and published for which Bnlearn includes two methods: ICA-based LiNGAM <a class="footnote-reference brackets" href="#id6" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, DirectLiNGAM <a class="footnote-reference brackets" href="#id7" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. The following three are not included VAR-LiNGAM <a class="footnote-reference brackets" href="#id8" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, RCD <a class="footnote-reference brackets" href="#id9" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, and CAM-UV <a class="footnote-reference brackets" href="#id10" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id6" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Shimizu, S., Hoyer, P. O., Hyvarinen, A., Kerminen, A., &amp; Jordan, M. (2006). A linear non-Gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10).</p>
</aside>
<aside class="footnote brackets" id="id7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Shimizu, S., Inazumi, T., Sogawa, Y., Hyvarinen, A., Kawahara, Y., Washio, T., … &amp; Bollen, K. (2011). DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model. The Journal of Machine Learning Research, 12, 1225-1248.</p>
</aside>
<aside class="footnote brackets" id="id8" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Hyvarinen, A., Zhang, K., Shimizu, S., &amp; Hoyer, P. O. (2010). Estimation of a structural vector autoregression model using non-gaussianity. Journal of Machine Learning Research, 11(5).</p>
</aside>
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Maeda, T. N., &amp; Shimizu, S. (2020, June). RCD: Repetitive causal discovery of linear non-Gaussian acyclic models with latent confounders. In International Conference on Artificial Intelligence and Statistics (pp. 735-745). PMLR.</p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Maeda, T. N., &amp; Shimizu, S. (2021). Causal Additive Models with Unobserved Variables. UAI.</p>
</aside>
</aside>
<p>To demonstrate how the LiNGAM works, it is best to do it with a small toy example. Let’s create test data containing six variables.</p>
<p>The goal of this dataset is to demonstrate the contribution of different variables and their causal impact on other variables.
All variables must be consistent, as in any other dataset. The sample size is set to n=1000 with a uniform distribution.
If the number of samples is much smaller, say in the tens, the method becomes less reliable due to insufficient information to determine causality.</p>
<p>We will establish dependencies between variables and then allow the model to infer the original values.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Step 1: <code class="docutils literal notranslate"><span class="pre">x3</span></code> is the root node and is initialized with a uniform distribution.</p></li>
<li><p>Step 2: <code class="docutils literal notranslate"><span class="pre">x0</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> are created by multiplying with the values of <code class="docutils literal notranslate"><span class="pre">x3</span></code>, making them dependent on <code class="docutils literal notranslate"><span class="pre">x3</span></code>.</p></li>
<li><p>Step 3: <code class="docutils literal notranslate"><span class="pre">x5</span></code> is created by multiplying with the values of <code class="docutils literal notranslate"><span class="pre">x0</span></code>, making it dependent on <code class="docutils literal notranslate"><span class="pre">x0</span></code>.</p></li>
<li><p>Step 4: <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x4</span></code> are created by multiplying with the values of <code class="docutils literal notranslate"><span class="pre">x0</span></code>, making them dependent on <code class="docutils literal notranslate"><span class="pre">x0</span></code>.</p></li>
</ol>
</div></blockquote>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig8a" src="_images/fig_lingam_example_input.png" /></p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">lingam.utils</span> <span class="kn">import</span> <span class="n">make_dot</span>

<span class="c1"># Number of samples</span>
<span class="n">n</span><span class="o">=</span><span class="mi">1000</span>

<span class="c1"># step 1</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="c1"># step 2</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mf">3.0</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mf">6.0</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="c1"># step 3</span>
<span class="n">x5</span> <span class="o">=</span> <span class="mf">4.0</span><span class="o">*</span><span class="n">x0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="c1"># step 4</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mf">3.0</span><span class="o">*</span><span class="n">x0</span> <span class="o">+</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">x4</span> <span class="o">=</span> <span class="mf">8.0</span><span class="o">*</span><span class="n">x0</span> <span class="o">-</span> <span class="mf">1.0</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">x4</span><span class="p">,</span> <span class="n">x5</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x0&#39;</span><span class="p">,</span> <span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;x3&#39;</span><span class="p">,</span> <span class="s1">&#39;x4&#39;</span><span class="p">,</span> <span class="s1">&#39;x5&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]])</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">make_dot</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">dot</span>
</pre></div>
</div>
<p>Structure learning can be applied with the direct-lingam method for fitting.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">structure_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">methodtype</span><span class="o">=</span><span class="s1">&#39;direct-lingam&#39;</span><span class="p">)</span>

<span class="c1"># When we no look at the output, we can see that the dependency values are very well recovered for the various variables.</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;adjmat&#39;</span><span class="p">])</span>
<span class="c1"># target        x0        x1       x2   x3        x4       x5</span>
<span class="c1"># source</span>
<span class="c1"># x0      0.000000  2.987320  0.00000  0.0  8.057757  3.99624</span>
<span class="c1"># x1      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>
<span class="c1"># x2      0.000000  2.010043  0.00000  0.0 -0.915306  0.00000</span>
<span class="c1"># x3      2.971198  0.000000  5.98564  0.0 -0.704964  0.00000</span>
<span class="c1"># x4      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>
<span class="c1"># x5      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Compute edge strength with the chi_square test statistic</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;adjmat&#39;</span><span class="p">])</span>
<span class="c1"># target        x0        x1       x2   x3        x4       x5</span>
<span class="c1"># source</span>
<span class="c1"># x0      0.000000  2.987320  0.00000  0.0  8.057757  3.99624</span>
<span class="c1"># x1      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>
<span class="c1"># x2      0.000000  2.010043  0.00000  0.0 -0.915306  0.00000</span>
<span class="c1"># x3      2.971198  0.000000  5.98564  0.0 -0.704964  0.00000</span>
<span class="c1"># x4      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>
<span class="c1"># x5      0.000000  0.000000  0.00000  0.0  0.000000  0.00000</span>

<span class="c1"># Using the causal_order_ properties, we can see the causal ordering as a result of the causal discovery.</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;causal_order&#39;</span><span class="p">])</span>
<span class="c1"># [&#39;x3&#39;, &#39;x0&#39;, &#39;x5&#39;, &#39;x2&#39;, &#39;x1&#39;, &#39;x4&#39;]</span>

<span class="c1"># We can draw a causal graph by utility funciton.</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig7a" src="_images/fig_lingam_example_1.png" /></p></td>
</tr>
</tbody>
</table>
<p>This example nicely demonstrates that we can capture the dependencies with the causal factors acurately.</p>
</section>
<section id="direct-lingam-method">
<h2>Direct-LiNGAM method<a class="headerlink" href="#direct-lingam-method" title="Link to this heading"></a></h2>
<p>The Direct-LiNGAM method ‘direct-lingam’ is a semi-parametric approach that assumes a linear relationship among observed variables while ensuring that the error terms follow a non-Gaussian distribution, with the constraint that the graph remains acyclic. This method involves repeated regression analysis and independence assessments using linear regression with least squares. In each regression, one variable serves as the dependent variable (outcome), while the other acts as the independent variable (predictor). This process is applied to each type of variable. When regression analysis is conducted in the correct causal order, the independent variables and error terms will exhibit independence. Conversely, if the regression is performed under an incorrect causal order, the independence of the explanatory variables and error terms is disrupted. By leveraging the dependency properties (where both residuals and explanatory variables share common error terms), it becomes possible to infer the causal order among the variables. Furthermore, for a given observed variable, any explanatory variable that remains independent of the residuals, regardless of the other variables considered, can be inferred as the first in the causal hierarchy.</p>
<p>Or in other words, the lingam-direct method allows you to model continuous and mixed datasets.
A disadvantage is that causal discovery of structure learning is the end-point when uing this method. It is not possible to perform parameter learning and inferences.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="nn">bn</span>

<span class="c1"># Load data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">import_example</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;auto_mpg&#39;</span><span class="p">)</span>
<span class="k">del</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;origin&#39;</span><span class="p">]</span>

<span class="c1"># Structure learning</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">structure_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">methodtype</span><span class="o">=</span><span class="s1">&#39;direct-lingam&#39;</span><span class="p">,</span> <span class="n">params_lingam</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;random_state&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">})</span>

<span class="c1"># Compute edge strength</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Plot with graphviz</span>
<span class="n">dotgraph</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">dotgraph</span>
<span class="n">dotgraph</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;dotgraph_auto_mpg_lingam_direct&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the LINGAM method, the values on the edges describe the dependency using a multiplication factor of one variable to another. As an example, Origin -&gt; -10 -&gt; Displacement tells us Displacement has values that are factor -10 lower than origin.</p>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig4a" src="_images/fig_auto_mpg_lingam_a.png" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="fig4b" src="_images/fig_auto_mpg_lingam_b.png" /></p></td>
</tr>
</tbody>
</table>
</section>
<section id="ica-lingam-method">
<h2>ICA-LiNGAM method<a class="headerlink" href="#ica-lingam-method" title="Link to this heading"></a></h2>
<p>The ICA-LiNGAM method ‘ica-lingam’ is also from lingam and follows the same procedure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="nn">bn</span>

<span class="c1"># Load data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">import_example</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;auto_mpg&#39;</span><span class="p">)</span>
<span class="k">del</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;origin&#39;</span><span class="p">]</span>


<span class="c1"># Structure learning</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">structure_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">methodtype</span><span class="o">=</span><span class="s1">&#39;ica-lingam&#39;</span><span class="p">)</span>

<span class="c1"># Compute edge strength</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">prune</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Plot with graphviz</span>
<span class="n">dotgraph</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">dotgraph</span>
<span class="n">dotgraph</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;dotgraph_auto_mpg_lingam_ica&#39;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig6a" src="_images/fig_auto_mpg_lingam_ica_a.png" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="fig6b" src="_images/fig_auto_mpg_lingam_ica_b.png" /></p></td>
</tr>
</tbody>
</table>
</section>
<section id="pc-method">
<h2>PC method<a class="headerlink" href="#pc-method" title="Link to this heading"></a></h2>
<p>A different, but quite straightforward approach to build a DAG from data is identifing independencies in the data set using hypothesis tests and then construct DAG (pattern) according to identified independencies (Conditional) Independence Tests. Independencies in the data can be identified using chi2 conditional independence tests.</p>
<p>The Constraint-Based PC Algorithm (named after Peter and Clark, its inventors) is a popular method in causal inference and Bayesian network learning. It is a type of constraint-based algorithm, which uses conditional independence tests to build a causal graph from data. This algorithm is widely used to learn the structure of Bayesian networks and causal graphs by identifying relationships between variables.</p>
<p>DAG (pattern) construction
With a method for independence testing at hand, we can construct a DAG from the data set in three steps:</p>
<blockquote>
<div><ul class="simple">
<li><ol class="arabic simple">
<li><p>Construct an undirected skeleton.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>Orient compelled edges to obtain partially directed acyclid graph.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>Extend DAG pattern to a DAG by conservatively orienting the remaining edges in some way.</p></li>
</ol>
</li>
</ul>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import</span>
<span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="nn">bn</span>

<span class="c1"># Load data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">import_example</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="s1">&#39;auto_mpg&#39;</span><span class="p">)</span>

<span class="c1"># Structure learning</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">structure_learning</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">methodtype</span><span class="o">=</span><span class="s1">&#39;pc&#39;</span><span class="p">)</span>

<span class="c1"># Compute edge strength</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">independence_test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">bn</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>

<span class="c1"># Plot with graphviz</span>
<span class="n">dotgraph</span> <span class="o">=</span> <span class="n">bn</span><span class="o">.</span><span class="n">plot_graphviz</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">edge_labels</span><span class="o">=</span><span class="s1">&#39;pvalue&#39;</span><span class="p">)</span>
<span class="n">dotgraph</span>
<span class="n">dotgraph</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;dotgraph_auto_mpg_PC&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>PC PDAG construction is only guaranteed to work under the assumption that the identified set of independencies is <em>faithful</em>, i.e. there exists a DAG that exactly corresponds to it. Spurious dependencies in the data set can cause the reported independencies to violate faithfulness. It can happen that the estimated PDAG does not have any faithful completions (i.e. edge orientations that do not introduce new v-structures). In that case a warning is issued.</p>
<table class="docutils align-center">
<tbody>
<tr class="row-odd"><td><p><img alt="fig5a" src="_images/fig_auto_mpg_PC_a.png" /></p></td>
</tr>
<tr class="row-even"><td><p><img alt="fig5b" src="_images/fig_auto_mpg_PC_b.png" /></p></td>
</tr>
</tbody>
</table>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Link to this heading"></a></h3>
<blockquote>
<div><p>1. Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer (2015),
<a class="reference external" href="https://arxiv.org/abs/1512.02406">Learning Discrete Bayesian Networks from Continuous Data</a></p>
<ol class="arabic simple" start="2">
<li><p>Julia 0.4 implementation:
<a class="reference external" href="https://github.com/sisl/LearnDiscreteBayesNets.jl">https://github.com/sisl/LearnDiscreteBayesNets.jl</a></p></li>
</ol>
</div></blockquote>
<hr>
<center>
        <script async src="https://media.ethicalads.io/media/client/ethicalads.min.js"></script>
        <!-- Show an image ad -->
        <!-- <div data-ea-publisher="erdogantgithubio" data-ea-type="image"></div> -->
        <div data-ea-publisher="erdogantgithubio" data-ea-type="image" data-ea-style="stickybox"></div>
</center>
<hr></section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Inference.html" class="btn btn-neutral float-left" title="Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Predict.html" class="btn btn-neutral float-right" title="Predict" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Erdogan Taskesen.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>